{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-data-validation==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify project-specific constants bracketed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP project id\n",
    "PROJECT_ID = '<project_id>'\n",
    "# BQ dataset id\n",
    "DATASET_ID = '<dataset_id>'\n",
    "# dataset location \n",
    "DATA_LOCATION = '<location>' # e.g. \"EU\"\n",
    "# source table name to extract sample from \n",
    "SOURCE_TABLE = '<training_input_table_id>' \n",
    "# sample table id suffix\n",
    "SAMPLE_TABLE = SOURCE_TABLE + '<table_suffix>' # e.g. \"_sample\"\n",
    "# full source table name\n",
    "SOURCE_TABLE_WITH_PROJECT_ID = PROJECT_ID + '.' + DATASET_ID + '.' + SOURCE_TABLE\n",
    "# full sample table names \n",
    "SAMPLE_TABLE_WITH_PROJECT_ID = PROJECT_ID + '.' + DATASET_ID + '.' + SAMPLE_TABLE\n",
    "SAMPLE_TABLE_WO_PROJECT_ID = DATASET_ID + '.' + SAMPLE_TABLE\n",
    "\n",
    "# GCS path to schema folder \n",
    "GCS_BASE_DIR = 'gs://<bucket>/<schema_folder>/'\n",
    "# GCS table name\n",
    "GCS_TABLE = '<sample_table.csv>'\n",
    "# full GCS sample table path\n",
    "GCS_SAMPLE_TABLE = GCS_BASE_DIR + GCS_TABLE\n",
    "# sample table size\n",
    "# should be defined such that the sample table provides adequate representation of distributions of numeric features\n",
    "SAMPLE_SIZE = 1000 \n",
    "\n",
    "# name of label column for serving \n",
    "LABEL_COLUMN_NAME = '<label_column_name>' # e.g. \"total_fare\"\n",
    "# features to check for skew with corresponding threshold values\n",
    "SKEW_THRESHOLD = {\n",
    "  \"<feature_1>\": 0.01,\n",
    "  \"<feature_2>\": 0.01\n",
    "}\n",
    "# the threshold values should be defined in \n",
    "# L infinity norm for categorical features\n",
    "# jensen shannon divergence for numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1> Create a sample of the preprocessed dataset </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a subset from the source table and write to a sample table in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bq --location=$DATA_LOCATION query \\\n",
    "--use_legacy_sql=false \\\n",
    "--destination_table=$SAMPLE_TABLE_WITH_PROJECT_ID \\\n",
    "--replace=true \\\n",
    "'CREATE OR REPLACE TABLE {SAMPLE_TABLE_WITH_PROJECT_ID} AS (SELECT * FROM `{SOURCE_TABLE_WITH_PROJECT_ID}` LIMIT {SAMPLE_SIZE})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extract the sample to GCS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the sample table to a cloud bucket as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=$DATA_LOCATION extract \\\n",
    "$SAMPLE_TABLE_WO_PROJECT_ID \\\n",
    "$GCS_SAMPLE_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generate statistics from sample</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate statistics from the csv file and write to `sample_stats.pb` in the same bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_csv(\n",
    "    data_location=GCS_SAMPLE_TABLE,\n",
    "    output_path=GCS_BASE_DIR+'sample_stats.pb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer schema from sample statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample table should provide an adequate representation of the numeric features in terms of data schema. However, it would only contain a small portion of all possible domain values of categorical features due to its size. Therefore, the full domains of categorical features need to be generated from the original (preprocessed) dataset. The domain values in the schema then need to be cleared and re-populated with the new values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify schema to inclue full string domain of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each categorical feature\n",
    "for f in schema.string_domain:\n",
    "    # take the schema of feature f to be modified\n",
    "    # clear existing domain of f\n",
    "    while len(f.value)>0:\n",
    "        f.value.pop()\n",
    "    \n",
    "    # query full domain values from original data\n",
    "    QUERY = (f'SELECT DISTINCT {f.name} FROM `{SOURCE_TABLE_WITH_PROJECT_ID}` WHERE {f.name} IS NOT NULL')\n",
    "    query_job = bigquery.Client().query(QUERY)\n",
    "    rows = query_job.result()\n",
    "    \n",
    "    # append full list of values to the cleared domain of f\n",
    "    for row in rows:\n",
    "        new_value = list(row.values())\n",
    "        f.value.extend(new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write schema to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to `tfdv_schema_training.pbtxt` in the same bucker folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.write_schema_text(\n",
    "    schema=schema,\n",
    "    output_path=GCS_BASE_DIR+'tfdv_schema_training.pbtxt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generate serving schema based on the training schema</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify skew comparators for serving data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of names of categorical features\n",
    "cat_feat = [f.name for f in schema.string_domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add skew comparator for features in SKEW_THRESHOLD\n",
    "for feature in SKEW_THRESHOLD:\n",
    "    f = tfdv.get_feature(schema, feature)\n",
    "    if feature in cat_feat:\n",
    "        # use infinity_norm for categorical features\n",
    "        f.skew_comparator.infinity_norm.threshold = SKEW_THRESHOLD[feature]\n",
    "    else:\n",
    "        # use jensen_shannon_divergence for numeric features\n",
    "        f.skew_comparator.jensen_shannon_divergence.threshold = SKEW_THRESHOLD[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify pipeline environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TRAINING and SERVING environments\n",
    "schema.default_environment.append('TRAINING')\n",
    "schema.default_environment.append('SERVING')\n",
    "\n",
    "# specify that the label column is not in SERVING environment\n",
    "tfdv.get_feature(schema, LABEL_COLUMN_NAME).not_in_environment.append('SERVING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the modified schema to `tfdv_schema_serving.pbtxt` in the same bucker folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.write_schema_text(\n",
    "    schema=schema,\n",
    "    output_path=GCS_BASE_DIR+'tfdv_schema_serving.pbtxt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Add schema to ../assets</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `tfdv_schema_training.pbtxt` and `tfdv_schema_serving.pbtxt` to the `../assets` folder. Make sure the files are named correctly. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
