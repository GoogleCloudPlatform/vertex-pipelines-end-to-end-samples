{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff4f7db-3a60-4872-9648-c76f8112a8a8",
   "metadata": {},
   "source": [
    "# TFMA Model Evaluation Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c1ddf-9d55-4903-8a18-8d8fea0ea979",
   "metadata": {},
   "source": [
    "This Notebook will guide the user as to how to obtain embedded HTML visualisations of TFMA model evaluation metrics used and created during training. This Notebook must be run after the completion of the training pipeline. \n",
    "\n",
    "The following steps are required:\n",
    "1. Install necessary packages \n",
    "2. Define path to predictions file in GCS and desired metrics to evaluate\n",
    "3. Run TFMA evaluation\n",
    "4. Obtain HTML files to visualise\n",
    "\n",
    "<span style=\"color:red\">*Disclaimer:*</span> This Notebook is meant to be run as a Vertex AI Workbench within the GCP environment. If you wish to run this Notebook locally you would need to:\n",
    "1. Download the `predictions` file you wish to evaluate from GCS into your local machine\n",
    "2. Replace the `csv_file` variable to point to the local path instead\n",
    "3. Download the `<custom_metric_name>.py` custom metric you wish to use from GCS into your local machine. Save these files in the same folder as this Notebook.\n",
    "4. Comment out the `Custom Metrics` section of the Notebook.\n",
    "5. Run the rest of the Notebook as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a0643-36e7-410b-b3f1-7bd2cb7e9830",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e6c85d-9cbe-46c2-a20c-40cbb75ec06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_model_analysis==0.37.0 pandas==1.3.5 google_cloud_storage==1.43.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a2d14-42ed-40e3-8764-4abf0bf84298",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da0cab7-0d72-4654-b45a-8e68a730f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 15:18:47.904516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 15:18:47.904578: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation-specific imports\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tensorflow_model_analysis.view import render_slicing_metrics\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# TFMA Evaluation\n",
    "import pandas as pd\n",
    "from google.protobuf import text_format\n",
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70acd24-9689-4d85-9142-eb01d2a4d28d",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428830ee-9587-490c-9332-778d77d9207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the link to the predictions generated during the training pipeline, which are stored in GCS. These are the output of the \"Predict Test Data\"\n",
    "component, and are saved in an Dataset Artefact called \"predictions\", which then act as the input to the \"Evaluate test metrics for <challenger>/<champion> model\"\n",
    "component\n",
    "\"\"\"\n",
    "csv_file = 'gs://alvaro-sandbox/pipeline_root/805011877165/tensorflow-train-pipeline-20220223132851/predict-tensorflow-model_-2494514806493544448/predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193a43e8-7ead-425d-bec9-d494b90ebaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_name = \"total_fare\" # Label column name (this is the ground truth)\n",
    "pred_column_name = \"predictions\" # Model prediction column name\n",
    "\n",
    "metrics_names = [\"MeanSquaredError\"] # Metric used to evaluate the model. Could be more than one ([\"MeanSquaredError\", \"<metric_name>\"]\n",
    "custom_metrics = {\"SquaredPearson\": \"squared_pearson\"} # Custom metric used to evaluate the model. If None used, leave it as custom_metrics = {}. If more \n",
    "                                                        # than use used, then custom_metrics = {\"SquaredPearson\": \"squared_pearson\", <\"MetricName\">:<\"module_name\">}\n",
    "\n",
    "# Slicing types used during evaluation. If no slicing used, leave it as slicing_specs = []\n",
    "slicing_specs=[\n",
    "        'feature_keys: [\"payment_type\"]',\n",
    "        'feature_keys: [\"payment_type\", \"company\"]',\n",
    "        'feature_values: [{key: \"payment_type\", value: \"Cash\"}]',\n",
    "        'feature_keys: [\"company\", \"dayofweek\"] feature_values: [{key: \"payment_type\", value:  \"Cash\"}]',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201f02c3-c2c8-4992-a092-072cc2400f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location to pipeline assets. Used only if custom metrics are available\n",
    "PIPELINE_FILES_GCS_PATH='gs://alvaro-sandbox/pipelines'\n",
    "VERTEX_PROJECT_ID='datatonic-vertex-pipeline-dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ebb5cf-82da-4994-ab61-afd98074f8e2",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d76a5d-b867-4d3d-b986-9458dc3506f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded custom metric module squared_pearson.py to Notebook storage\n"
     ]
    }
   ],
   "source": [
    "# The custom metric module must be downloaded from GCS where it is being stored.\n",
    "# If no custom metrics are used, this cell won't run anything.\n",
    "\n",
    "if custom_metrics:\n",
    "\n",
    "    custom_metrics_path = f\"{PIPELINE_FILES_GCS_PATH}/training/assets/tfma_custom_metrics\"\n",
    "\n",
    "    storage_client = storage.Client(project=VERTEX_PROJECT_ID)\n",
    "    for custom_metric in custom_metrics.values():\n",
    "        with open(f\"{custom_metric}.py\", \"wb\") as fp:\n",
    "            storage_client.download_blob_to_file(f\"{custom_metrics_path}/{custom_metric}.py\", fp)\n",
    "    \n",
    "    for custom_metric in custom_metrics.values():\n",
    "        assert f\"{custom_metric}.py\" in os.listdir(), f\"Custom Metric module {custom_metric}.py could not be found at {custom_metrics_path}\"\n",
    "        \n",
    "        print(f\"Downloaded custom metric module {custom_metric}.py to Notebook storage\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"No custom metrics were specified by the user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e22f57-36f3-43ba-8a11-16121294395f",
   "metadata": {},
   "source": [
    "## Define TFMA model evaluation specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc1e065-36b8-433b-9eb6-04c4f8a7dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_specs {\n",
      "  label_key: \"total_fare\"\n",
      "  prediction_key: \"predictions\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"payment_type\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"payment_type\"\n",
      "  feature_keys: \"company\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_values {\n",
      "    key: \"payment_type\"\n",
      "    value: \"Cash\"\n",
      "  }\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"company\"\n",
      "  feature_keys: \"dayofweek\"\n",
      "  feature_values {\n",
      "    key: \"payment_type\"\n",
      "    value: \"Cash\"\n",
      "  }\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"MeanSquaredError\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"SquaredPearson\"\n",
      "    module: \"squared_pearson\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file) # Read predictions and convert to dataframe\n",
    "\n",
    "# Iterate through all metrics\n",
    "metrics_specs = \"\"\n",
    "for metric in metrics_names:\n",
    "    metrics_specs += f'metrics {{ class_name: \"{metric}\" }}\\n'\n",
    "\n",
    "# Adding custom metrics if specified\n",
    "if custom_metrics:\n",
    "    for class_name, module_name in custom_metrics.items():\n",
    "        metric_spec = f' {{ class_name: \"{class_name}\" module: \"{module_name}\" }}'\n",
    "        metrics_specs += f\"metrics {metric_spec}\\n\"\n",
    "\n",
    "# Iterate through all slices\n",
    "slicing_spec_proto = \"slicing_specs {}\\n\"\n",
    "if slicing_specs:\n",
    "    for single_slice in slicing_specs:\n",
    "        slicing_spec_proto += f\"slicing_specs {{ {single_slice} }}\\n\"\n",
    "\n",
    "# Create evaluation configuration\n",
    "protobuf = \"\"\"\n",
    "            ## Model information\n",
    "            model_specs {{\n",
    "                label_key: \"{0}\"\n",
    "                prediction_key: \"{1}\"\n",
    "            }}\n",
    "            ## Post export metric information\n",
    "            metrics_specs {{\n",
    "                {2}\n",
    "            }}\n",
    "            ## Slicing information inc. overall\n",
    "            {3}\n",
    "            \"\"\"\n",
    "\n",
    "eval_config = text_format.Parse(\n",
    "    protobuf.format(\n",
    "        label_column_name, pred_column_name, metrics_specs, slicing_spec_proto\n",
    "    ),\n",
    "    tfma.EvalConfig(),\n",
    ")\n",
    "\n",
    "print(eval_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59850e-192c-4daf-a9aa-650b96ecd29c",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b34ee-21db-41ea-bf8c-9cce62cc51bb",
   "metadata": {},
   "source": [
    "This will save the results of the TFMA evaluation under a file called `eval_outputs` which is created by TFMA itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58a83fa-2757-4a84-afb7-8fd090590092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 15:18:51.981807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-14 15:18:51.981860: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-14 15:18:51.981893: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tfma-visualisations-final): /proc/driver/nvidia/version does not exist\n",
      "2022-03-14 15:18:51.982214: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.analyze_raw_data(df, eval_config=eval_config, output_path=\"eval_outputs/\")\n",
    "evaluation = eval_result.get_metrics_for_all_slices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ad013-ae35-4944-b614-89cdcb9aff4b",
   "metadata": {},
   "source": [
    "## Save Evaluation in HTML Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7092e2-bcbd-4ca4-9efa-e7b71be84a59",
   "metadata": {},
   "source": [
    "This will save the HTML plots under a file called `html_outputs`. It will create one file for every slice specified in `slicing_specs`, as well as a plot with the overall metrics, without any slice specified. For example, if \n",
    "\n",
    "```\n",
    "slicing_specs=[\n",
    "        'feature_keys: [\"payment_type\"]',\n",
    "        'feature_keys: [\"payment_type\", \"company\"]',\n",
    "        'feature_values: [{key: \"payment_type\", value: \"Cash\"}]',\n",
    "        'feature_keys: [\"company\", \"dayofweek\"] feature_values: [{key: \"payment_type\", value: \"Cash\"}]',\n",
    "    ]\n",
    "```\n",
    "four HTML files will be created, as follows:\n",
    "1. `feature_keys: [\"payment_type\"]` will show the metrics for all of the different `payment_type` values available\n",
    "2. `feature_keys: [\"payment_type\", \"company\"]` will show the metrics for every unique combination of `payment_type` and `company` values available\n",
    "3. `feature_values: [{key: \"payment_type\", value: \"Cash\"}]` will show the metrics only for the cases where the `payment_type` is `Cash`\n",
    "4. `feature_keys: [\"company\", \"dayofweek\"] feature_values: [{key: \"payment_type\", value: \"Cash\"}]` will show the metrics for every unique combination of `company` and `dayofweek` wherever the `payment_type` is `Cash`.\n",
    "\n",
    "Additionally, a fifth plot would be created, which contains the metrics with no slice applied.\n",
    "\n",
    "Once the plots are created, to view and interact with them, double click on the file you wish to open. This will open a new tab with the name of the plot. Then click on `Trust HTML` and wait for a few seconds to see the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26ad16a-4e59-426a-b91e-3b344b7f9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_value_pair(key_value_string):\n",
    "    \"\"\"String manipulation to obtain the key-value pair from the slicing specification. Currently TFMA only\n",
    "        supports having a single key-value pair as part of a slicing specification. If this changes, this \n",
    "        function must also change.\n",
    "    \n",
    "    Args:\n",
    "        key_value_string (str): String containing the key-value pair. This string has the following naming convention:\n",
    "            'feature_keys: [\"<feature_key>\"] feature_values: [{key: \"<key>\", value: \"<value>\"}]'. The string\n",
    "            manipulation aims to obtain the <key> and <value> names.\n",
    "            \n",
    "    Returns:\n",
    "        key (str): Key name given in slicing spec.\n",
    "        value (str): Value name given in slicing spec.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get key name\n",
    "    key = key_value_string\\\n",
    "                        .split(\"key:\")[1]\\\n",
    "                        .split(\",\")[0]\\\n",
    "                        .replace('\"',\"\")\\\n",
    "                        .replace(\"'\",\"\")\\\n",
    "                        .strip()\n",
    "    \n",
    "    # Get value name\n",
    "    value = key_value_string\\\n",
    "                        .split(\"value:\")[1]\\\n",
    "                        .split(\"}\")[0]\\\n",
    "                        .replace('\"',\"\")\\\n",
    "                        .replace(\"'\",\"\")\\\n",
    "                        .strip()\n",
    "    return key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024fa19f-1df1-4bcd-a12f-d09c7af73318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_keys(keys_string):\n",
    "    \"\"\"String manipulation to obtain all feature keys from a single slicing specification returned as a single list\n",
    "    \n",
    "    Args:\n",
    "        keys_string (str): String containing the feature keys. This string has the following naming convention:\n",
    "            'feature_keys: [\"<feature_one>\", \"<feature_two>\"]'. The string manipulation aims to obtain \n",
    "            all of the <feature_XX> keys in a single list\n",
    "            \n",
    "    Returns:\n",
    "        feature_keys (list): List containing all feature keys in the given slice\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_keys = [] # Initialise empty list\n",
    "    \n",
    "    # Get all keys as list of string\n",
    "    \"\"\"\n",
    "    Need to convert string 'feature_keys: [\"<feature_one>\", \"<feature_two>\"]' \n",
    "    into list of strings [\"<feature_one>\", \"<feature_two>\"]\n",
    "    \"\"\"\n",
    "    keys_list = keys_string\\\n",
    "                        .split(\"feature_keys:\")[1]\\\n",
    "                        .lstrip()\\\n",
    "                        .split(\"[\")[1]\\\n",
    "                        .split(\"]\")[0]\\\n",
    "                        .split(\",\")\n",
    "    \n",
    "    # Clean every string item in list\n",
    "    for onekey in keys_list:\n",
    "            keyname = onekey.replace('\"',\"\").replace(\"'\",\"\").strip()\n",
    "            feature_keys.append(keyname)\n",
    "            \n",
    "    return feature_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4997da63-1466-45b6-bc6c-13d96f5e8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"html_outputs/\", exist_ok=True) # Save files in this local folder\n",
    "\n",
    "# Create an output file fore very slice type\n",
    "for onespec in slicing_specs:\n",
    "    \n",
    "    # If only feature keys are specified\n",
    "    if \"feature_keys:\" in onespec and \"feature_values: \" not in onespec:\n",
    "        spec_keys = get_feature_keys(onespec) # Get all keys as list of strings\n",
    "        specs = tfma.SlicingSpec(feature_keys=spec_keys) # Create slicing spec\n",
    "        plots_tfma = render_slicing_metrics(eval_result, slicing_spec=specs) # Plot metrics\n",
    "        embed_minimal_html(f'html_outputs/plots_{\"_&_\".join(spec_keys)}.html', views=[plots_tfma], title='Slicing Metrics')\n",
    "        \n",
    "    # If only feature values are specified\n",
    "    elif \"feature_values: \" in onespec and \"feature_keys:\" not in onespec:\n",
    "        keyname, valname = get_key_value_pair(onespec) # Get key-value pair names\n",
    "        specs = tfma.SlicingSpec(feature_values={keyname:valname}) # Create slicing spec\n",
    "        plots_tfma = render_slicing_metrics(eval_result, slicing_spec=specs) # Plot metrics\n",
    "        embed_minimal_html(f'html_outputs/plots_{keyname}_-->_{valname}.html', views=[plots_tfma], title='Slicing Metrics')\n",
    "    \n",
    "    # If a combination of feature keys and values are specified\n",
    "    elif \"feature_keys:\" in onespec and \"feature_values: \" in onespec:\n",
    "        keyname, valname = get_key_value_pair(onespec) # Get key-value pair names\n",
    "        spec_keys = get_feature_keys(onespec) # Get all keys as list of strings\n",
    "        specs = tfma.SlicingSpec(feature_keys=spec_keys, \n",
    "                                 feature_values={keyname:valname}) # Create slicing spec\n",
    "        plots_tfma = render_slicing_metrics(eval_result, slicing_spec=specs) # Plot metrics\n",
    "        embed_minimal_html(f'html_outputs/plots_{\"_&_\".join(spec_keys)}_<>_{keyname}_-->_{valname}.html', views=[plots_tfma], title='Slicing Metrics')\n",
    "\n",
    "# Create a final plot without any slice, just for the overall metric\n",
    "plots_tfma = render_slicing_metrics(eval_result)\n",
    "embed_minimal_html(f'html_outputs/plots_overall.html', views=[plots_tfma], title='Slicing Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8a7d7-f9d6-45b3-bbad-9c3b28494b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
