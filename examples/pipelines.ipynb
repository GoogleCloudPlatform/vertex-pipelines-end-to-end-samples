{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Running Vertex Pipelines using End-to-end Samples repository.\n",
    "\n",
    "{TODO: Update the links below.} \n",
    "\n",
    "{TODO: ðŸ”´ Potentially remove colab and workbench as terraform installation could be difficult?}\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows you how to run production ready pipelines on Google Cloud using Datatonic's Vertex Pipelines End-to-end Samples repository.\n",
    "\n",
    "Learn more about [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to set up the repo, launch your first training and predicition pipeline, and analyse the results:\n",
    "\n",
    "This tutorial uses the following Google Cloud services and resources:\n",
    "\n",
    "- *`Vertex Pipelines`*\n",
    "- *`Google Cloud Storage`*\n",
    "- *`Artifact Registry`*\n",
    "- *`BigQuery`*\n",
    "- *`Cloud Build`*\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "* Deploy infrastructure using Terraform for a typical setup of Vertex AI and other relevant services.\n",
    "* Run ML training and batch prediction pipelines using the Kubeflow Pipelines SDK for an example use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Example ML training and predictions pipelines for scikit-learn/XGBoost will use the popular [Chicago Taxi Trips Dataset](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=chicago_taxi_trips&page=dataset). The dataset includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency.\n",
    "\n",
    "\n",
    "This public dataset is hosted in Google BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "* Cloud Build\n",
    "* Artifact Registry\n",
    "\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "[BigQuery pricing](https://cloud.google.com/bigquery/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and [Cloud Build pricing](https://cloud.google.com/build/pricing),\n",
    "and [Artifact Registry](https://cloud.google.com/artifact-registry/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Pyenv](https://github.com/pyenv/pyenv#installation) for managing Python versions\n",
    "- [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/quickstart)\n",
    "- Make\n",
    "- [Poetry](https://python-poetry.org)\n",
    "- [Terraform](https://www.terraform.io) (To install Terraform on your local machine we recommend using [tfswitch](https://tfswitch.warrensbox.com/) to automatically choose and download an appropriate version.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clone Turbo Templates repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone a Git repository\n",
    "!git clone -b develop https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd vertex-pipelines-end-to-end-samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the correct Python version\n",
    "! pyenv install -skip-existing\n",
    "\n",
    "#Â configure poetry \n",
    "! poetry config virtualenvs.prefer-active-python true\n",
    "\n",
    "#Install poetry dependencies for ML pipelines\n",
    "! make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel. ðŸ”´{TODO: Potentially remove colab as terraform installation could be difficult?}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before You Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[my-project-id]\"  # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account {TODO: Potentially remove colab and workbench as terraform installation could be difficult?ðŸ”´ðŸ”´ðŸ”´}\n",
    "\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup {Work in progress}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run make commands relevant environment variables need to be set. Please create `env.sh`, and update the environment variables for your dev environment (particularly `VERTEX_PROJECT_ID`, `VERTEX_LOCATION` and `RESOURCE_SUFFIX`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using `os` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VERTEX_PROJECT_ID'] = 'my-project-id'\n",
    "os.environ['VERTEX_LOCATION'] = 'europe-west2'\n",
    "\n",
    "os.environ['VERTEX_CMEK_IDENTIFIER'] = ''  # optional\n",
    "os.environ['VERTEX_NETWORK'] = ''  # optional\n",
    "\n",
    "# Suffix (e.g. '<your name>') to facilitate running concurrent pipelines in the same Google Cloud project.\n",
    "# Change if working in a team to avoid overwriting resources during development \n",
    "os.environ['RESOURCE_SUFFIX'] = 'default'\n",
    "\n",
    "# Leave as-is\n",
    "os.environ['VERTEX_SA_EMAIL'] = f'vertex-pipelines@{os.environ[\"VERTEX_PROJECT_ID\"]}.iam.gserviceaccount.com'\n",
    "os.environ['VERTEX_PIPELINE_ROOT'] = f'gs://{os.environ[\"VERTEX_PROJECT_ID\"]}-pl-root'\n",
    "os.environ['CONTAINER_IMAGE_REGISTRY'] = f'{os.environ[\"VERTEX_LOCATION\"]}-docker.pkg.dev/{os.environ[\"VERTEX_PROJECT_ID\"]}/vertex-images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `os` library could suitable. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Env variables are available in shell commands.\n",
    "!echo $VERTEX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Env variables available for make commands.\n",
    "# ! make install\n",
    "! make compile pipeline=training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Env variables can be used in cells using os library.\n",
    "PROJECT_ID = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "TF_BUCKET_URI = f\"{PROJECT_ID}-tfstate\"\n",
    "print(TF_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using dotenv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "\n",
    "VERTEX_CMEK_IDENTIFIER= # optional\n",
    "VERTEX_LOCATION=europe-west2\n",
    "VERTEX_NETWORK= # optional\n",
    "VERTEX_PROJECT_ID=my-project-id-dot\n",
    "\n",
    "# Suffix (e.g. '<your name>') to facilitate running concurrent pipelines in the same Google Cloud project. Change if working in a team to avoid overwriting resources during development \n",
    "RESOURCE_SUFFIX=default\n",
    "\n",
    "# Leave as-is\n",
    "VERTEX_SA_EMAIL=vertex-pipelines@${VERTEX_PROJECT_ID}.iam.gserviceaccount.com\n",
    "VERTEX_PIPELINE_ROOT=gs://${VERTEX_PROJECT_ID}-pl-root\n",
    "CONTAINER_IMAGE_REGISTRY=${VERTEX_LOCATION}-docker.pkg.dev/${VERTEX_PROJECT_ID}/vertex-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Env variables are available in shell commands.\n",
    "!echo $VERTEX_PROJECT_ID\n",
    "!echo $CONTAINER_IMAGE_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Env variables available for make commands.\n",
    "# ! make install\n",
    "! make compile pipeline=training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Env variables can be used in cells using os library.\n",
    "import os\n",
    "PROJECT_ID = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "TF_BUCKET_URI = f\"{PROJECT_ID}-tfstate\"\n",
    "print(TF_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Writing `env.sh`\n",
    "\n",
    "This approach has some drawbacks ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile env.sh\n",
    "\n",
    "#!/bin/bash\n",
    "export VERTEX_CMEK_IDENTIFIER= # optional\n",
    "export VERTEX_LOCATION=europe-west2\n",
    "export VERTEX_NETWORK= # optional\n",
    "export VERTEX_PROJECT_ID=my-project-id-envsh\n",
    "\n",
    "# Suffix (e.g. '<your name>') to facilitate running concurrent pipelines in the same Google Cloud project. Change if working in a team to avoid overwriting resources during development \n",
    "export RESOURCE_SUFFIX=default\n",
    "\n",
    "# Leave as-is\n",
    "export VERTEX_SA_EMAIL=vertex-pipelines@${VERTEX_PROJECT_ID}.iam.gserviceaccount.com\n",
    "export VERTEX_PIPELINE_ROOT=gs://${VERTEX_PROJECT_ID}-pl-root\n",
    "export CONTAINER_IMAGE_REGISTRY=${VERTEX_LOCATION}-docker.pkg.dev/${VERTEX_PROJECT_ID}/vertex-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has some issues (see below): ðŸ”´ðŸ”´ðŸ”´ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Env variables are not available in shell commands. ðŸ”´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! echo $VERTEX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use env variables in a command you need to add %%bash and source env.sh into the cell everytime.\n",
    "# For example some of the steps in infrastructure deployment would look like:ðŸ”´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env.sh\n",
    "echo $VERTEX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env.sh\n",
    "gsutil mb -l $VERTEX_LOCATION -p $VERTEX_PROJECT_ID gs://$VERTEX_PROJECT_ID-tfstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env.sh\n",
    "terraform -chdir=terraform/envs/dev init -backend-config=\"bucket=${VERTEX_PROJECT_ID}-tfstate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make commands are working fine since it cointains:\n",
    "# -include env.sh\n",
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make compile pipeline=training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Env variables are not available for python scripts ðŸ”´\n",
    "import os\n",
    "PROJECT_ID = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "TF_BUCKET_URI = f\"{PROJECT_ID}-tfstate\"\n",
    "print(TF_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "## Infrastructure deployment using terraform.\n",
    "\n",
    "Please check if you have terraform installed. If not we recommend using [`tfswitch`](https://tfswitch.warrensbox.com/) to automatically choose and download an appropriate version for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tfstate bucket\n",
    "\n",
    "Before provisioning our infrastructure we need to create Google Cloud Storage (GCS) bucket that will be used to store the state files for Terraform deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $VERTEX_LOCATION -p $VERTEX_PROJECT_ID gs://$VERTEX_PROJECT_ID-tfstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Deploy required infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Backend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! terraform -chdir=terraform/envs/dev init -backend-config=\"bucket=${VERTEX_PROJECT_ID}-tfstate\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terraform Plan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! terraform -chdir=terraform/envs/dev plan -var \"project_id=${VERTEX_PROJECT_ID}\" -var \"region=${VERTEX_LOCATION}\" # -lock=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terraform Apply** Please check tf plan output to see what infrastructure will be provisioned. If everything looks fine uncomment and run cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! terraform -chdir=terraform/envs/dev apply -var \"project_id=${VERTEX_PROJECT_ID}\" -var \"region=${VERTEX_LOCATION}\" -auto-approve # -lock=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I've been getting state lock issues ðŸ”´ðŸ”´ðŸ”´  -lock=false fixes it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository contains example ML training and prediction pipelines for two popular frameworks (XGBoost/sklearn and Tensorflow) using the popular [Chicago Taxi Dataset](https://console.cloud.google.com/marketplace/details/city-of-chicago-public-data/chicago-taxi-trips). The details of these can be found in the [separate README](pipelines/README.md).\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "Before you can run these example pipelines successfully there are a few additional things you will need to deploy (they have not been included in the Terraform code as they are specific to these pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new BigQuery dataset for the Chicago Taxi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:chicago_taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new BigQuery dataset for data processing during the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a BigQuery transfer job to mirror the Chicago Taxi dataset to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-bigquery-datatransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "destination_project_id = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "destination_dataset_id = \"chicago_taxi_trips\"\n",
    "source_project_id = \"bigquery-public-data\"\n",
    "source_dataset_id = \"chicago_taxi_trips\"\n",
    "transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "    destination_dataset_id=destination_dataset_id,\n",
    "    display_name=\"Chicago taxi trip mirror\",\n",
    "    data_source_id=\"cross_region_copy\",\n",
    "    params={\n",
    "        \"source_project_id\": source_project_id,\n",
    "        \"source_dataset_id\": source_dataset_id,\n",
    "    },\n",
    "    schedule=\"every 24 hours\",\n",
    ")\n",
    "transfer_config = transfer_client.create_transfer_config(\n",
    "    parent=transfer_client.common_project_path(destination_project_id),\n",
    "    transfer_config=transfer_config,\n",
    ")\n",
    "TRANSFER_CONFIG=transfer_config.name\n",
    "print(f\"Created transfer config: {TRANSFER_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the container images\n",
    "\n",
    "The [model/](/model/) directory contains the code for custom training and serving container images [model/training/train.py](model/training/train.py).\n",
    "\n",
    "Build the training and serving container images and push them to Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running Pipelines\n",
    "\n",
    "You can run the training pipeline (for example) by executing cell below.\n",
    "\n",
    "This will start the pipeline using the chosen template on Vertex AI, namely it will:\n",
    "\n",
    "1. Compile the pipeline using the Kubeflow Pipelines SDK\n",
    "1. Trigger the pipeline with the help of `pipelines/trigger/main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make run pipeline=training build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Prediction Pipeline** \n",
    "\n",
    "After successful training run you can try prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make run pipeline=prediction build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## 3. Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Terraform Plan (destroy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! terraform -chdir=terraform/envs/dev plan -destroy -var \"project_id=${VERTEX_PROJECT_ID}\" -var \"region=${VERTEX_LOCATION}\" # -lock=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Terraform Destroy**\n",
    "\n",
    "ðŸ”´ðŸ”´ðŸ”´  Cant delete staging and root buckets because contains objects and `force_destroy` is not set to true in our terraform configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! terraform -chdir=terraform/envs/dev destroy -var \"project_id=${VERTEX_PROJECT_ID}\" -var \"region=${VERTEX_LOCATION}\" -auto-approve # -lock=false  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Delete data transfer config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.api_core.exceptions\n",
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "transfer_config_name = TRANSFER_CONFIG\n",
    "try:\n",
    "    transfer_client.delete_transfer_config(name=transfer_config_name)\n",
    "except google.api_core.exceptions.NotFound:\n",
    "    print(\"Transfer config not found.\")\n",
    "else:\n",
    "    print(f\"Deleted transfer config: {transfer_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Delete BigQuery dataset for the Chicago Taxi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} rm -f -r --dataset \"${VERTEX_PROJECT_ID}:chicago_taxi_trips\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Delete BigQuery dataset for data processing during the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} rm -f -r --dataset \"${VERTEX_PROJECT_ID}:preprocessing\" "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
