{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Vertex AI Turbo Templates\n",
    "\n",
    "In this notebook you'll run your first production-ready training and prediction pipelines on Google Cloud. Follow this three-part notebook series to get started in a local Jupyter notebook or in [Vertex AI Workbench](https://cloud.google.com/vertex-ai-notebooks):\n",
    "\n",
    "1. [Infrastructure Setup](./02_run_pipelines.ipynb)\n",
    "1. **[Run Pipelines](./02_run_pipelines.ipynb) - this notebook**\n",
    "1. [Infrastructure Clean Up](./02_run_pipelines.ipynb)\n",
    "\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- Deployed `dev` project\n",
    "- [Pyenv](https://github.com/pyenv/pyenv#installation) for managing Python versions\n",
    "- [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/quickstart)\n",
    "- Make\n",
    "- [Poetry](https://python-poetry.org)\n",
    "- [pyenv](https://github.com/pyenv/pyenv)\n",
    "\n",
    "**For Vertex AI Workbench users**: Uncomment and execute the following cell install Poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! bash ./scripts/install_poetry.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate\n",
    "\n",
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_PROJECT_ID = \"my-project-id\"\n",
    "! gcloud auth login\n",
    "! gcloud config set project {VERTEX_PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF",
    "tags": []
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install Python and Poetry dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd vertex-pipelines-end-to-end-samples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pyenv install -skip-existing\n",
    "! poetry config virtualenvs.prefer-active-python true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Makefile` installs virtual Python environments for the Vertex AI Pipelines and components within using Poetry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Pipeline\n",
    "\n",
    "Vertex AI Pipelines uses KubeFlow to orchestrate your training steps, as such you'll need to:\n",
    "\n",
    "1. Compile the pipeline\n",
    "1. Build dependent Docker containers\n",
    "1. Run the pipeline in Vertex AI\n",
    "\n",
    "The already templated training pipeline will execute a pipeline similar to the image below in Vertex AI:\n",
    "\n",
    "![Training Pipeline](../images/training_pipeline.png)\n",
    "\n",
    "Don't worry about executing steps 1-3 manually (and each time you run your pipeline!), simply run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make training wait=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the pipeline executes, here's a more detailed explanation of what's happening:\n",
    "\n",
    "**1. Compile the pipeline:** By using the KubeFlow SDK, you've compiled the training pipeline in `pipelines/training` to YAML.\n",
    "\n",
    "**2. Build dependent Docker containers:** In `model` you can maintain your training (and serving) code which is containerised and pushed to [Artifact Registry](https://cloud.google.com/artifact-registry). \n",
    "In this way, your training pipeline can execute your training code in the `Train model` pipeline step.\n",
    "\n",
    "**3. Run the pipeline in Vertex AI:** By using the Vertex AI Python SDK and the pipeline YAML file, you execute your training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prediction Pipeline\n",
    "\n",
    "After running a successful training pipeline job, run the prediction pipeline which will look similar to:\n",
    "\n",
    "<img src=\"../images/prediction_pipeline.png\" alt=\"image\" width=\"500\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make prediction build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note:** The command has the following true/false flags:\n",
    "\n",
    "- `build` - re-build containers for training & serving code (limit by setting `images=training` to build only one of the containers)\n",
    "- `compile` - re-compile the pipeline to YAML\n",
    "- `wait` - run the pipeline (a-)sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You've successfully run your first training and prediction pipeline in Vertex AI! ðŸŽ‰\n",
    "Continue with [this notebook](./03_infrastructure_cleanup.ipynb) to run clean up the infrastructure."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
