{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Running Vertex Pipelines using E2E Samples repository - Triggering Pipelines.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines_colab.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines_workbench.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows you how to run production ready pipelines on Google Cloud using Datatonic's Vertex Pipelines End-to-end Samples repository.\n",
    "\n",
    "Learn more about [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to launch your first training and predicition pipeline, and analyse the results:\n",
    "\n",
    "This tutorial uses the following Google Cloud services and resources:\n",
    "\n",
    "- *`Vertex Pipelines`*\n",
    "- *`Google Cloud Storage`*\n",
    "- *`Artifact Registry`*\n",
    "- *`BigQuery`*\n",
    "- *`Cloud Build`*\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "* Run ML training and batch prediction pipelines using the Kubeflow Pipelines SDK for an example use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Example ML training and predictions pipelines for scikit-learn/XGBoost will use the popular [Chicago Taxi Trips Dataset](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=chicago_taxi_trips&page=dataset). The dataset includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency.\n",
    "\n",
    "\n",
    "This public dataset is hosted in Google BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "* Cloud Build\n",
    "* Artifact Registry\n",
    "\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "[BigQuery pricing](https://cloud.google.com/bigquery/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and [Cloud Build pricing](https://cloud.google.com/build/pricing),\n",
    "and [Artifact Registry](https://cloud.google.com/artifact-registry/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Pyenv](https://github.com/pyenv/pyenv#installation) for managing Python versions\n",
    "- [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/quickstart)\n",
    "- Make\n",
    "- [Poetry](https://python-poetry.org)\n",
    "- [pyenv](https://github.com/pyenv/pyenv)\n",
    "- Infrastructure deployed ([01_infrastructure_setup.ipynb](01_infrastructure_setup.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd vertex-pipelines-end-to-end-samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF",
    "tags": []
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the correct Python version\n",
    "! pyenv install -skip-existing\n",
    "\n",
    "#Â configure poetry \n",
    "! poetry config virtualenvs.prefer-active-python true\n",
    "\n",
    "#Install poetry dependencies for ML pipelines\n",
    "! make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**If you do not have terraform installed please follow the instructions below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local JupyterLab instance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow official documentation for installation instructions: https://developer.hashicorp.com/terraform/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before You Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa",
    "tags": []
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[my-project-id]\"  # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "\n",
    "As you're running a Jupyter environment locally, you'll need to authenticate manually. Please follow the instructions provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run make commands relevant environment variables need to be set. You can re-use `.env` file defined in [01_infrastructure_setup.ipynb](01_infrastructure_setup.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables using [Python-dotenv](https://pypi.org/project/python-dotenv/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To automate, monitor, and govern your ML workflows, you can use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-mlops). Vertex AI is a powerful platform offered by Google Cloud that empowers organizations to streamline and enhance their Machine Learning (ML) workflows through automation, monitoring, and governance.\n",
    "\n",
    "- **Automation**: Vertex AI offers a suite of tools and services designed to automate various aspects of ML development and deployment. This includes automating data preprocessing, model training, hyperparameter tuning, and model deployment. Automation not only saves time but also reduces the potential for human error, making your ML workflows more efficient and reliable.\n",
    "\n",
    "- **Monitoring**: Effective monitoring is crucial for maintaining the performance and reliability of ML models in production. Vertex AI provides monitoring capabilities that allow you to track model performance, detect drift in data distributions, and set up alerts for anomalies. This proactive monitoring ensures that your models continue to deliver accurate results as data and business conditions change over time.\n",
    "\n",
    "- **Governance**: Managing ML models and data in a secure and compliant manner is essential for businesses, especially those in regulated industries. Vertex AI helps you implement governance policies to control access to data, monitor model usage, and enforce compliance with data privacy regulations. This ensures that your ML operations are in line with legal and ethical standards.\n",
    "\n",
    "- **Scalability**: Vertex AI is built on Google Cloud's infrastructure, which means it offers unparalleled scalability. Whether you're dealing with small-scale experiments or large-scale production deployments, Vertex AI can scale to meet your needs, ensuring that your ML workflows can handle increased workloads without performance bottlenecks.\n",
    "\n",
    "- **Collaboration**: Collaboration is essential in ML development, and Vertex AI provides features that facilitate collaboration among data scientists, machine learning engineers, and other stakeholders. You can share notebooks, collaborate on model development, and maintain version control of your ML assets.\n",
    "\n",
    "- **Model Serving**: Vertex AI makes it easy to deploy ML models as APIs for real-time inference or batch processing. This enables you to integrate your ML models into applications, websites, or other services with ease.\n",
    "\n",
    "- **Cost Management**: Cost control is a crucial aspect of any ML project. Vertex AI offers cost management tools and insights to help you optimize your ML workflows and keep your expenses in check.\n",
    "\n",
    "By utilizing Vertex AI, you can take advantage of Google Cloud's cutting-edge technology and expertise in machine learning to accelerate your MLOps journey. The platform offers a comprehensive set of tools and services that cover the entire ML lifecycle, from data preparation to model deployment and beyond, making it a valuable resource for organizations looking to harness the power of ML in a scalable, efficient, and secure manner.\n",
    "\n",
    "To learn more about MLOps on Vertex AI and how it can transform your ML workflows, you can visit the [official documentation](https://cloud.google.com/vertex-ai/docs/start/introduction-mlops).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**This repository provides an example ML training and prediction pipelines for XGBoost using the popular [Chicago Taxi Dataset](https://console.cloud.google.com/marketplace/details/city-of-chicago-public-data/chicago-taxi-trips).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "Before you can successfully execute the example pipelines, there are a few additional components you need to deploy. These components have not been included in the Terraform code as they are specific to these pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new BigQuery dataset for the Chicago Taxi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:chicago_taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new BigQuery dataset for data processing during the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a BigQuery transfer job to mirror the Chicago Taxi dataset to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-bigquery-datatransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "destination_project_id = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "destination_dataset_id = \"chicago_taxi_trips\"\n",
    "source_project_id = \"bigquery-public-data\"\n",
    "source_dataset_id = \"chicago_taxi_trips\"\n",
    "transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "    destination_dataset_id=destination_dataset_id,\n",
    "    display_name=\"Chicago taxi trip mirror\",\n",
    "    data_source_id=\"cross_region_copy\",\n",
    "    params={\n",
    "        \"source_project_id\": source_project_id,\n",
    "        \"source_dataset_id\": source_dataset_id,\n",
    "    },\n",
    "    schedule=\"every 24 hours\",\n",
    ")\n",
    "transfer_config = transfer_client.create_transfer_config(\n",
    "    parent=transfer_client.common_project_path(destination_project_id),\n",
    "    transfer_config=transfer_config,\n",
    ")\n",
    "TRANSFER_CONFIG=transfer_config.name\n",
    "print(f\"Created transfer config: {TRANSFER_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the container images\n",
    "\n",
    "The `model` directory contains the code for custom training and serving container images `model/training/train.py`.\n",
    "\n",
    "A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. To learn more you can check out [official documentation](https://cloud.google.com/vertex-ai/docs/training/containers-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the training and serving container images and push them to Artifact Registry run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running Pipelines\n",
    "\n",
    "You can run the training pipeline by executing cell below.\n",
    "\n",
    "This will start the pipeline using the chosen template on Vertex AI, namely it will:\n",
    "\n",
    "1. Compile the pipeline using the Kubeflow Pipelines SDK\n",
    "1. Trigger the pipeline with the help of `pipelines/trigger/main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make run pipeline=training build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After executing the command, a link will appear, leading you to the Vertex AI platform, where you can monitor your pipeline job. Alternatively, you can access it directly through the Google Cloud Console UI by navigating to Vertex AI and then selecting Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training Pipeline**\n",
    "![Training Pipeline](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/docs/images/training_pipeline.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Prediction Pipeline** \n",
    "\n",
    "After successful training job, you can try prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make run pipeline=prediction build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Prediction Pipeline**\n",
    "\n",
    "![Predictions Pipeline](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/docs/images/prediction_pipeline.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Congratulations on successfully running your first ML pipelines!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
