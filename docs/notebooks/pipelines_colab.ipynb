{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Running Vertex Pipelines using E2E Samples repository.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines_colab.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/examples/pipelines_workbench.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows you how to run production ready pipelines on Google Cloud using Datatonic's Vertex Pipelines End-to-end Samples repository.\n",
    "\n",
    "Learn more about [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to set up the repo, launch your first training and predicition pipeline, and analyse the results:\n",
    "\n",
    "This tutorial uses the following Google Cloud services and resources:\n",
    "\n",
    "- *`Vertex Pipelines`*\n",
    "- *`Google Cloud Storage`*\n",
    "- *`Artifact Registry`*\n",
    "- *`BigQuery`*\n",
    "- *`Cloud Build`*\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "* Deploy infrastructure using Terraform for a typical setup of Vertex AI and other relevant services.\n",
    "* Run ML training and batch prediction pipelines using the Kubeflow Pipelines SDK for an example use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Example ML training and predictions pipelines for scikit-learn/XGBoost will use the popular [Chicago Taxi Trips Dataset](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=chicago_taxi_trips&page=dataset). The dataset includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency.\n",
    "\n",
    "\n",
    "This public dataset is hosted in Google BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "* Cloud Build\n",
    "* Artifact Registry\n",
    "\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "[BigQuery pricing](https://cloud.google.com/bigquery/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and [Cloud Build pricing](https://cloud.google.com/build/pricing),\n",
    "and [Artifact Registry](https://cloud.google.com/artifact-registry/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXI8WcMpuyXI",
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Pyenv](https://github.com/pyenv/pyenv#installation) for managing Python versions\n",
    "- [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/quickstart)\n",
    "- Make\n",
    "- [Poetry](https://python-poetry.org)\n",
    "- [Terraform](https://www.terraform.io) (To install Terraform on your local machine we recommend using [tfswitch](https://tfswitch.warrensbox.com/) to automatically choose and download an appropriate version.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujfOJ2gHuyXJ",
    "tags": []
   },
   "source": [
    "## Clone Turbo Templates repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5YwqQbDuyXJ",
    "outputId": "9b703837-a177-4956-8c77-2985dea88601",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone a Git repository\n",
    "!git clone -b develop https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlBC9B_OuyXJ",
    "outputId": "00a3fd6f-b06d-461d-8a24-a37d0351e095",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd vertex-pipelines-end-to-end-samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF",
    "tags": []
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1hZ7k5guyXJ"
   },
   "source": [
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hdfngMuyXJ",
    "outputId": "1233d17f-8cd2-4185-9b84-3defadc42669"
   },
   "outputs": [],
   "source": [
    "# @title Prepare notebook.\n",
    "# @markdown 1. Configure pyenv\n",
    "!curl https://pyenv.run | bash\n",
    "\n",
    "# Add pyenv to PATH\n",
    "import os\n",
    "os.environ[\"PATH\"] += \":/root/.pyenv/bin\"\n",
    "\n",
    "# Initialize pyenv\n",
    "!eval \"$(pyenv init -)\"\n",
    "!eval \"$(pyenv virtualenv-init -)\"\n",
    "\n",
    "! pip install poetry\n",
    "\n",
    "# @markdown 2. Install the correct Python version\n",
    "! pyenv install -skip-existing\n",
    "\n",
    "#Â @markdown 3. Configure poetry\n",
    "! poetry config virtualenvs.prefer-active-python true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKr_J0wY6_Pr"
   },
   "source": [
    "#### Install poetry dependencies for ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrev7zxh6GCt",
    "outputId": "68e48bbd-e646-42f1-d315-2557059807f3"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JuYvvx8yuyXK",
    "outputId": "12f0622d-0362-4cd9-e333-0df6f7cc01a1"
   },
   "outputs": [],
   "source": [
    "# @title Installing Terraform by downloading a pre-compiled binary.\n",
    "# @markdown This cell will:\n",
    "# @markdown 1. **Download** the appropriate package for your system as a zip archive from the Terraform website.\n",
    "# @markdown 2. **Unzip** the downloaded package.\n",
    "# @markdown 3. **Ensure Terraform is on your PATH**.\n",
    "\n",
    "%%bash\n",
    "mkdir temp\n",
    "cd temp\n",
    "curl -so terraform.zip https://releases.hashicorp.com/terraform/1.5.7/terraform_1.5.7_linux_amd64.zip\n",
    "unzip terraform.zip > /dev/null && rm -f terraform.zip > /dev/null\n",
    "mv terraform /usr/local/bin > /dev/null\n",
    "cd ..\n",
    "rm -r temp\n",
    "terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: execute the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f200f10a1da3",
    "outputId": "6c870257-e089-41a3-ed89-c70b311eb7b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D31uhP4V9lTc",
    "outputId": "1ffe618d-962f-4efc-d1b5-dd82c976733b"
   },
   "outputs": [],
   "source": [
    "%cd vertex-pipelines-end-to-end-samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj8s3i90uyXK",
    "tags": []
   },
   "source": [
    "## Before You Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa",
    "tags": []
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "8d1edaa5-d9ea-407e-efea-fbdb9691f60f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"my-project-id\"  # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRCfV5duuyXK"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "\n",
    "Follow the relevant instructions below to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY-Kz5hmuyXL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5nZH6H5uyXL"
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znu_gBnkuyXL"
   },
   "source": [
    "In order to run make commands relevant environment variables need to be set. Please update the environment variables for your dev environment (particularly `VERTEX_PROJECT_ID`, `VERTEX_LOCATION` and `RESOURCE_SUFFIX`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfN37cNEuyXL",
    "outputId": "4e20a0a9-6551-428a-ab2e-ac22b530b5f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "\n",
    "VERTEX_LOCATION=europe-west2\n",
    "VERTEX_PROJECT_ID=my-project-id\n",
    "\n",
    "# Suffix (e.g. '<your name>') to facilitate running concurrent pipelines in the same Google Cloud project. Change if working in a team to avoid overwriting resources during development \n",
    "RESOURCE_SUFFIX=default\n",
    "\n",
    "# Leave as-is\n",
    "VERTEX_SA_EMAIL=vertex-pipelines@${VERTEX_PROJECT_ID}.iam.gserviceaccount.com\n",
    "VERTEX_PIPELINE_ROOT=gs://${VERTEX_PROJECT_ID}-pl-root\n",
    "CONTAINER_IMAGE_REGISTRY=${VERTEX_LOCATION}-docker.pkg.dev/${VERTEX_PROJECT_ID}/vertex-images\n",
    "\n",
    "# Optional\n",
    "VERTEX_CMEK_IDENTIFIER=\n",
    "VERTEX_NETWORK="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EC2VyhYvuyXL",
    "outputId": "ac399c93-78e2-4bc8-c789-32b49809a789",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1iwILB7uyXN",
    "outputId": "efef313e-03b5-4b92-b528-3d92084b39b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk",
    "tags": []
   },
   "source": [
    "## Infrastructure deployment using terraform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMbeEpCuuyXN"
   },
   "source": [
    "#### Enable the Cloud Resource Manager and Service Usage APs for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqu8hNAluyXN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable cloudresourcemanager.googleapis.com --project=$VERTEX_PROJECT_ID\n",
    "! gcloud services enable serviceusage.googleapis.com --project=$VERTEX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0W1RMZluyXN"
   },
   "source": [
    "#### Create tfstate bucket\n",
    "\n",
    "Before provisioning your infrastructure we need to create Google Cloud Storage (GCS) bucket that will be used to store the [state files](https://developer.hashicorp.com/terraform/language/state/remote) remotely for Terraform deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIq7R4HZCfIc",
    "outputId": "c9e11df0-587a-4bb6-92ac-254fc1a78085",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $VERTEX_LOCATION -p $VERTEX_PROJECT_ID gs://$VERTEX_PROJECT_ID-tfstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Deploy required infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "330YqgzfuyXN",
    "tags": []
   },
   "source": [
    "Deploy command will:\n",
    "1. Prepare a Terraform working directory by downloading any necessary provider plugins and initialize the backend configuration.\n",
    "1. Create infrastructure resources defined in Terraform configuration (terraform/envs/dev)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-LPegqLYuyXN",
    "outputId": "ea4a2b99-d729-4f94-a4a0-67ac883246f0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "make deploy auto-approve=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytXz5BkNuyXN",
    "tags": []
   },
   "source": [
    "## Example ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2BLhXBpuyXN",
    "tags": []
   },
   "source": [
    "To automate, monitor, and govern your ML workflows, you can use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-mlops). Vertex AI is a powerful platform offered by Google Cloud that empowers organizations to streamline and enhance their Machine Learning (ML) workflows through automation, monitoring, and governance.\n",
    "\n",
    "- **Automation**: Vertex AI offers a suite of tools and services designed to automate various aspects of ML development and deployment. This includes automating data preprocessing, model training, hyperparameter tuning, and model deployment. Automation not only saves time but also reduces the potential for human error, making your ML workflows more efficient and reliable.\n",
    "\n",
    "- **Monitoring**: Effective monitoring is crucial for maintaining the performance and reliability of ML models in production. Vertex AI provides monitoring capabilities that allow you to track model performance, detect drift in data distributions, and set up alerts for anomalies. This proactive monitoring ensures that your models continue to deliver accurate results as data and business conditions change over time.\n",
    "\n",
    "- **Governance**: Managing ML models and data in a secure and compliant manner is essential for businesses, especially those in regulated industries. Vertex AI helps you implement governance policies to control access to data, monitor model usage, and enforce compliance with data privacy regulations. This ensures that your ML operations are in line with legal and ethical standards.\n",
    "\n",
    "- **Scalability**: Vertex AI is built on Google Cloud's infrastructure, which means it offers unparalleled scalability. Whether you're dealing with small-scale experiments or large-scale production deployments, Vertex AI can scale to meet your needs, ensuring that your ML workflows can handle increased workloads without performance bottlenecks.\n",
    "\n",
    "- **Collaboration**: Collaboration is essential in ML development, and Vertex AI provides features that facilitate collaboration among data scientists, machine learning engineers, and other stakeholders. You can share notebooks, collaborate on model development, and maintain version control of your ML assets.\n",
    "\n",
    "- **Model Serving**: Vertex AI makes it easy to deploy ML models as APIs for real-time inference or batch processing. This enables you to integrate your ML models into applications, websites, or other services with ease.\n",
    "\n",
    "- **Cost Management**: Cost control is a crucial aspect of any ML project. Vertex AI offers cost management tools and insights to help you optimize your ML workflows and keep your expenses in check.\n",
    "\n",
    "By utilizing Vertex AI, you can take advantage of Google Cloud's cutting-edge technology and expertise in machine learning to accelerate your MLOps journey. The platform offers a comprehensive set of tools and services that cover the entire ML lifecycle, from data preparation to model deployment and beyond, making it a valuable resource for organizations looking to harness the power of ML in a scalable, efficient, and secure manner.\n",
    "\n",
    "To learn more about MLOps on Vertex AI and how it can transform your ML workflows, you can visit the [official documentation](https://cloud.google.com/vertex-ai/docs/start/introduction-mlops).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fw_DSPaouyXN",
    "tags": []
   },
   "source": [
    "**This repository provides an example ML training and prediction pipelines for XGBoost using the popular [Chicago Taxi Dataset](https://console.cloud.google.com/marketplace/details/city-of-chicago-public-data/chicago-taxi-trips).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX4_OKX5uyXN"
   },
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "Before you can successfully execute the example pipelines, there are a few additional components you need to deploy. These components have not been included in the Terraform code as they are specific to these pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m66vkt7PuyXN"
   },
   "source": [
    "1. Create a new BigQuery dataset for the Chicago Taxi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBgpwdluuyXN",
    "outputId": "64044c15-bb91-4b27-973d-1914face2964",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:chicago_taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ88x_8xuyXO"
   },
   "source": [
    "2. Create a new BigQuery dataset for data processing during the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPWQoXP5uyXO",
    "outputId": "e968f292-6026-43a3-a9a4-637ffa8e5043",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} mk --dataset \"${VERTEX_PROJECT_ID}:preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTCqhzKOuyXO"
   },
   "source": [
    "3. Set up a BigQuery transfer job to mirror the Chicago Taxi dataset to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "mr9wTo8-uyXO",
    "outputId": "1f1aeb1d-869f-41db-8701-b45c1aca81a4"
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-bigquery-datatransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "EqI6dBIGuyXO",
    "outputId": "36281530-93bd-43da-cc1f-e72377651eaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "destination_project_id = os.environ[\"VERTEX_PROJECT_ID\"]\n",
    "destination_dataset_id = \"chicago_taxi_trips\"\n",
    "source_project_id = \"bigquery-public-data\"\n",
    "source_dataset_id = \"chicago_taxi_trips\"\n",
    "transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "    destination_dataset_id=destination_dataset_id,\n",
    "    display_name=\"Chicago taxi trip mirror\",\n",
    "    data_source_id=\"cross_region_copy\",\n",
    "    params={\n",
    "        \"source_project_id\": source_project_id,\n",
    "        \"source_dataset_id\": source_dataset_id,\n",
    "    },\n",
    "    schedule=\"every 24 hours\",\n",
    ")\n",
    "transfer_config = transfer_client.create_transfer_config(\n",
    "    parent=transfer_client.common_project_path(destination_project_id),\n",
    "    transfer_config=transfer_config,\n",
    ")\n",
    "TRANSFER_CONFIG=transfer_config.name\n",
    "print(f\"Created transfer config: {TRANSFER_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRtAwjcHuyXO"
   },
   "source": [
    "### Building the container images\n",
    "\n",
    "The `model` directory contains the code for custom training and serving container images `model/training/train.py`.\n",
    "\n",
    "A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. To learn more you can check out [official documentation](https://cloud.google.com/vertex-ai/docs/training/containers-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWj89EXmuyXO"
   },
   "source": [
    "To build the training and serving container images and push them to Artifact Registry run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu-E6Pn3uyXO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpJhQCwvuyXO",
    "tags": []
   },
   "source": [
    "### Running Pipelines\n",
    "\n",
    "You can run the training pipeline by executing cell below.\n",
    "\n",
    "This will start the pipeline using the chosen template on Vertex AI, namely it will:\n",
    "\n",
    "1. Compile the pipeline using the Kubeflow Pipelines SDK\n",
    "1. Trigger the pipeline with the help of `pipelines/trigger/main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUnltoeDuyXO"
   },
   "source": [
    "**Run Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwXpPsNPuyXO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make run pipeline=training build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a2Rz8tNuyXO",
    "tags": []
   },
   "source": [
    "After executing the command, a link will appear, leading you to the Vertex AI platform, where you can monitor your pipeline job. Alternatively, you can access it through the Google Cloud Console UI under Vertex AI/Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1yzYZQSuyXO",
    "tags": []
   },
   "source": [
    "**Training Pipeline**\n",
    "![Training Pipeline](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/docs/images/training_pipeline.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb1kLnZtuyXO"
   },
   "source": [
    "**Run Prediction Pipeline**\n",
    "\n",
    "After successful training job, you can try prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gM-UTYzuyXO"
   },
   "outputs": [],
   "source": [
    "! make run pipeline=prediction build=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYeHlnV4uyXO",
    "tags": []
   },
   "source": [
    "**Prediction Pipeline**\n",
    "\n",
    "![Predictions Pipeline](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples/blob/develop/docs/images/prediction_pipeline.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTCem_rKuyXO"
   },
   "source": [
    "**Empty the buckets to enable their deletion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW1WVh4_uyXO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil rm -a gs://${VERTEX_PROJECT_ID}-staging/**\n",
    "gsutil rm -a gs://${VERTEX_PROJECT_ID}-pl-root/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uXsfDFZuyXP",
    "tags": []
   },
   "source": [
    "**Delete data transfer config:**\n",
    "\n",
    "*If transfer config name was not set automatically you can find it via Google Cloud Console UI (BigQuery/Data Transfers)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5jdzMZeuyXP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.api_core.exceptions\n",
    "from google.cloud import bigquery_datatransfer\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "transfer_config_name = TRANSFER_CONFIG # Transfer Config Name\n",
    "try:\n",
    "    transfer_client.delete_transfer_config(name=transfer_config_name)\n",
    "except google.api_core.exceptions.NotFound:\n",
    "    print(\"Transfer config not found.\")\n",
    "else:\n",
    "    print(f\"Deleted transfer config: {transfer_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69WPFNN-uyXP",
    "tags": []
   },
   "source": [
    "**Delete BigQuery dataset for the Chicago Taxi data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcQ5cguluyXP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} rm -f -r --dataset \"${VERTEX_PROJECT_ID}:chicago_taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSamp3p9uyXP",
    "tags": []
   },
   "source": [
    "**Delete BigQuery dataset for data processing during the pipelines:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRKSZmozuyXP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! bq --location=${VERTEX_LOCATION} rm -f -r --dataset \"${VERTEX_PROJECT_ID}:preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhYbRJ0NuyXP",
    "tags": []
   },
   "source": [
    "**Delete local files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Msb_7QyLuyXP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -r vertex-pipelines-end-to-end-samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKDx-NvyuyXO",
    "tags": []
   },
   "source": [
    "**Destroy infrastructure by executing undeploy command (terraform destroy):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! make undeploy auto-approve=true"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd2d2b37aae7c0649f55b7e5d509b0b6c853821eea0be4d0f279f57c5531e1e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
