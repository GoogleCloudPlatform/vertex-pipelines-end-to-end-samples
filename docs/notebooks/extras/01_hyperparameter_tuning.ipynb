{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Vertex AI Turbo Templates\n",
    "\n",
    "This notebook guides you through production-ready pipelines on Google Cloud with hyperparameter tuning enabled. \n",
    "If you're new to the Vertex AI Turbo Template, start with the [three-part notebook series](../01_infrastructure_setup.ipynb) first. \n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- deployed cloud project\n",
    "- cloned template\n",
    "- `env.sh` configured\n",
    "- Python dependencies installed\n",
    "\n",
    "Let's change into the root of the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "**Update Python dependencies:** Add the following line to `model/pyproject.toml` to install (cloudml-hypertune)[https://github.com/GoogleCloudPlatform/cloudml-hypertune] as part of your model training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd model && poetry add cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extend training code:** Add hyperparameters which require tuning to the expected command-line arguments `model/training/train.py`.\n",
    "In this example we're adding the `learning_rate` hyperparamter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\n",
    "    \"--learning_rate\", type=float, default=0.3, help=\"Learning rate (default: 0.3)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report evaluation metric:** Don't forget to report the primary evaluation metric `rootMeanSquaredError` once the model is trained and evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypertune\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag=\"rootMeanSquaredError\",\n",
    "    metric_value=hp_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Python imports:** Add the `HyperparameterTuningJobRunOp` to your training pipeline in `pipelines/src/pipelines/training.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.hyperparameter_tuning_job import HyperparameterTuningJobRunOp as hpt_job_op\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.v1 import hyperparameter_tuning_job as hpt_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adapting the training pipeline, we'll need to implement three new components:\n",
    "\n",
    "- **get_worker_pool_specs:** Generate a specification for worker pools as a necessary input for performing hyperparameter tuning.\n",
    "- **hpt_job_op:** This runs the actual job. This components doesn't need to be implemented as it's already provided by [Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction).\n",
    "- **get_trials:** Retrieve all trials and their evaluation metrics after a successful hyperparameter tuning job. \n",
    "- **get_best_trial:** Get the best trial from all trials. This allows to understand which hyperparameters have led to the best model and also to retrieve the trained model artifact.\n",
    "\n",
    "Your updated training pipeline will look similar to this Vertex AI pipeline once implemented:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../../images/hparam_tuning.png\" alt=\"image\" width=\"250\" height=\"auto\">\n",
    "</p>\n",
    "\n",
    "Now add the following components to your training pipeline code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.9\")\n",
    "def get_worker_pool_specs(\n",
    "    input_data: Input[Dataset],\n",
    "    arguments: dict,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generate a specification for worker pools to perform hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        input_data (Input[Dataset]): Input data for training\n",
    "        hparams (dict): Hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        list: A list of worker pool specifications, each specifying the machine type,\n",
    "        Docker image, and command arguments for a worker pool.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    COMMAND = [\"python\"]\n",
    "    ARGS = [\n",
    "        \"-m\", \"training\",\n",
    "        \"--input_path\", input_data.path,\n",
    "    ]\n",
    "    for key, value in arguments.items():\n",
    "        ARGS.extend([\"--\" + str(key), str(value)])\n",
    "\n",
    "    return [{\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAINING_IMAGE,\n",
    "            \"command\": COMMAND,\n",
    "            \"args\": ARGS,\n",
    "        },\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-pipeline-components\",\n",
    "        \"protobuf\",\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def get_trials(gcp_resources: str) -> list:\n",
    "    \"\"\"Retrieve all trials after a successful hyperparameters tuning job.\n",
    "\n",
    "    Args:\n",
    "        gcp_resources (str): Proto tracking the hyperparameter tuning job.\n",
    "\n",
    "    Returns:\n",
    "        List of strings representing the intermediate JSON representation of the\n",
    "        trials from the hyperparameter tuning job.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.cloud.aiplatform_v1.types import study\n",
    "\n",
    "    api_endpoint_suffix = \"-aiplatform.googleapis.com\"\n",
    "    gcp_resources_proto = Parse(gcp_resources, GcpResources())\n",
    "    gcp_resources_split = gcp_resources_proto.resources[0].resource_uri.partition(\n",
    "        \"projects\"\n",
    "    )\n",
    "    resource_name = gcp_resources_split[1] + gcp_resources_split[2]\n",
    "    prefix_str = gcp_resources_split[0]\n",
    "    prefix_str = prefix_str[: prefix_str.find(api_endpoint_suffix)]\n",
    "    api_endpoint = prefix_str[(prefix_str.rfind(\"//\") + 2) :] + api_endpoint_suffix\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    job_client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    response = job_client.get_hyperparameter_tuning_job(name=resource_name)\n",
    "\n",
    "    return [study.Trial.to_json(trial) for trial in response.trials]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"google-cloud-aiplatform\"], base_image=\"python:3.9\")\n",
    "def get_best_trial(trials: list, study_spec_metrics: list) -> str:\n",
    "    \"\"\"Retrieves the best trial from the trials.\n",
    "\n",
    "    Args:\n",
    "        trials (list): Required. List representing the intermediate\n",
    "          JSON representation of the trials from the hyperparameter tuning job.\n",
    "        study_spec_metrics (list): Required. List serialized from dictionary\n",
    "          representing the metrics to optimize.\n",
    "          The dictionary key is the metric_id, which is reported by your training\n",
    "          job, and the dictionary value is the optimization goal of the metric\n",
    "          ('minimize' or 'maximize'). example:\n",
    "          metrics = hyperparameter_tuning_job.serialize_metrics(\n",
    "              {'loss': 'minimize', 'accuracy': 'maximize'})\n",
    "\n",
    "    Returns:\n",
    "        String representing the intermediate JSON representation of the best\n",
    "        trial from the list of trials.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If there are multiple metrics.\n",
    "    \"\"\"\n",
    "    from google.cloud.aiplatform_v1.types import study\n",
    "\n",
    "    if len(study_spec_metrics) > 1:\n",
    "        raise RuntimeError(\n",
    "            \"Unable to determine best parameters for multi-objective\"\n",
    "            \" hyperparameter tuning.\"\n",
    "        )\n",
    "    trials_list = [study.Trial.from_json(trial) for trial in trials]\n",
    "    best_trial = None\n",
    "    goal = study_spec_metrics[0][\"goal\"]\n",
    "    best_fn = None\n",
    "    if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n",
    "        best_fn = max\n",
    "    elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n",
    "        best_fn = min\n",
    "    best_trial = best_fn(\n",
    "        trials_list, key=lambda trial: trial.final_measurement.metrics[0].value\n",
    "    )\n",
    "\n",
    "    return study.Trial.to_json(best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training pipeline:** In the  define new variables and use the just implemented components by replacing the initial `train` component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"...\")\n",
    "def pipeline():\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # define static arguments which will be used for all tuning jobs\n",
    "    # `learning_rate` is excluded as we'll optimise this hyperparameter\n",
    "    static_arguments = dict(\n",
    "        n_estimators=200,\n",
    "        label=label,\n",
    "        # ...\n",
    "    )\n",
    "    # the metric(s) we want to optimise for\n",
    "    metrics = hpt_job.serialize_metrics({\"rootMeanSquaredError\": \"minimize\"})\n",
    "    \n",
    "    # prepare your data ...\n",
    "    \n",
    "    data_op = # ...\n",
    "    \n",
    "    # now use the new components ...\n",
    "    \n",
    "    specs_op = get_worker_pool_specs(\n",
    "        input_data=data_op.outputs[\"data\"],\n",
    "        arguments=training_arguments,\n",
    "    )\n",
    "    \n",
    "    tuning_op = hpt_job_op(\n",
    "        display_name=\"tuning-job\",\n",
    "        project=project,\n",
    "        location=location,\n",
    "        worker_pool_specs=specs_op.output,\n",
    "        \n",
    "        # TODO: adjust to your use case\n",
    "        base_output_directory=\"...\", # e.g. a folder in your pipeline bucket root\n",
    "        max_trial_count=3,           # e.g. run up to 3x trials\n",
    "        parallel_trial_count=3,      # e.g. run all of them in parallel\n",
    "        study_spec_algorithm=\"ALGORITHM_UNSPECIFIED\",\n",
    "        study_spec_measurement_selection_type=\"BEST_MEASUREMENT\",\n",
    "        study_spec_metrics=metrics,\n",
    "        \n",
    "        # TODO: specify the hyperparameters to be tuned\n",
    "        study_spec_parameters=hpt_job.serialize_parameters({\n",
    "            \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "        }),\n",
    "    )\n",
    "    \n",
    "    trials_op = get_trials(gcp_resources=tuning_op.outputs[\"gcp_resources\"])\n",
    "    \n",
    "    best_trial_op = get_best_trial(\n",
    "        trials=trials_op.output, \n",
    "        study_spec_metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    # TODO: upload the model with the best trial ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline\n",
    "\n",
    "Vertex AI Pipelines uses KubeFlow to orchestrate your training steps, as such you'll need to:\n",
    "\n",
    "1. Compile the pipeline\n",
    "2. Re-build the training container (as we've update the dependencies and code)\n",
    "3. Run the pipeline in Vertex AI\n",
    "\n",
    "Don't worry about executing steps 1-3 manually (and each time you run your pipeline!), simply run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make run pipeline=training build=true targets=training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully updated your training pipeline to support hyperparameter tuning! 🎉 \n",
    "\n",
    "Continue by reading more about [hyperparameter tuning in Vertex AI](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) or by improving your new training pipeline by uploading the model with the best evaluation result."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
