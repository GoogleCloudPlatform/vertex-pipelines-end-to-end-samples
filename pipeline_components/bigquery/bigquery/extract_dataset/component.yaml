name: Extract bq to dataset
description: Extract BQ table in GCS.
inputs:
- {name: bq_client_project_id, type: String, description: project id that will be
    used by the bq client}
- {name: source_project_id, type: String, description: project id from where BQ table
    will be extracted}
- {name: dataset_id, type: String, description: dataset id from where BQ table will
    be extracted}
- {name: table_name, type: String, description: table name (without project id and
    dataset id)}
- {name: dataset_location, type: String, description: bq dataset location. Defaults
    to "EU"., default: EU, optional: true}
- name: extract_job_config
  type: JsonObject
  description: |-
    dict containing optional parameters
    required by the bq extract operation. Defaults to None.
    See available parameters here
    https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa
  optional: true
- name: file_pattern
  type: String
  description: |-
    Exporting data into one or more files. If empty, then table data
    is exported to a single file. For multiple files and allowed pattern, see:
    https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_into_one_or_more_files # noqa
  optional: true
outputs:
- name: dataset
  type: Dataset
  description: |-
    output dataset artifact generated by the operation,
    this parameter will be passed automatically by the orchestrator
- {name: dataset_gcs_prefix, type: String}
- {name: dataset_gcs_uri, type: JsonArray}
implementation:
  container:
    image: python:3.7
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.30.0' 'kfp==1.8.9' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def extract_bq_to_dataset(
          bq_client_project_id: str,
          source_project_id: str,
          dataset_id: str,
          table_name: str,
          dataset: Output[Dataset],
          dataset_location: str = "EU",
          extract_job_config: dict = None,
          file_pattern: str = None,
      ) -> NamedTuple("Outputs", [("dataset_gcs_prefix", str), ("dataset_gcs_uri", list)]):
          """
          Extract BQ table in GCS.
          Args:
              bq_client_project_id (str): project id that will be used by the bq client
              source_project_id (str): project id from where BQ table will be extracted
              dataset_id (str): dataset id from where BQ table will be extracted
              table_name (str): table name (without project id and dataset id)
              dataset (Output[Dataset]): output dataset artifact generated by the operation,
                  this parameter will be passed automatically by the orchestrator
              dataset_location (str): bq dataset location. Defaults to "EU".
              extract_job_config (dict): dict containing optional parameters
                  required by the bq extract operation. Defaults to None.
                  See available parameters here
                  https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa
              file_pattern (str): Exporting data into one or more files. If empty, then table data
                  is exported to a single file. For multiple files and allowed pattern, see:
                  https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_into_one_or_more_files # noqa

          Returns:
              Outputs (NamedTuple (str, list)): Output dataset directory and its  GCS uri.
          """

          import logging
          import os
          from google.cloud.exceptions import GoogleCloudError
          from google.cloud import bigquery

          full_table_id = f"{source_project_id}.{dataset_id}.{table_name}"

          if extract_job_config is None:
              extract_job_config = {}

          table = bigquery.table.Table(table_ref=full_table_id)
          job_config = bigquery.job.ExtractJobConfig(**extract_job_config)
          client = bigquery.client.Client(
              project=bq_client_project_id, location=dataset_location
          )

          # if file_pattern is provided, join dataset.uri with file_pattern
          dataset_uri = dataset.uri
          if file_pattern:
              dataset_uri = os.path.join(dataset.uri, file_pattern)
          dataset_directory = os.path.dirname(dataset_uri)

          logging.info(f"Extract table {table} to {dataset_uri}")
          extract_job = client.extract_table(
              table,
              dataset_uri,
              job_config=job_config,
          )

          try:
              result = extract_job.result()
              logging.info("Table extracted, result: {}".format(result))
          except GoogleCloudError as e:
              logging.error(e)
              logging.error(extract_job.error_result)
              logging.error(extract_job.errors)
              raise e

          return (dataset_directory, [dataset_uri])

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - extract_bq_to_dataset
